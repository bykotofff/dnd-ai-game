// backend/src/modules/game-master/ollama.service.ts
import axios, { AxiosInstance } from 'axios'
import {
    OllamaConfig,
    AIModelConfig,
    AIMasterError,
    AIProcessingResult,
    AIResponse,
    AIRequestType
} from './ai-master.types'

export class OllamaService {
    private client: AxiosInstance
    private config: OllamaConfig
    private modelConfigs: Map<AIRequestType, AIModelConfig>

    constructor() {
        this.config = {
            baseUrl: process.env.OLLAMA_URL || 'http://localhost:11434',
            defaultModel: process.env.OLLAMA_MODEL || 'qwen2.5:14b',
            timeout: 60000, // 60 —Å–µ–∫—É–Ω–¥
            maxRetries: 3,
            models: {
                story: process.env.OLLAMA_STORY_MODEL || 'qwen2.5:14b',
                dialogue: process.env.OLLAMA_DIALOGUE_MODEL || 'qwen2.5:14b',
                combat: process.env.OLLAMA_COMBAT_MODEL || 'qwen2.5:7b',
                quest: process.env.OLLAMA_QUEST_MODEL || 'qwen2.5:14b'
            }
        }

        this.client = axios.create({
            baseURL: this.config.baseUrl,
            timeout: this.config.timeout,
            headers: {
                'Content-Type': 'application/json'
            }
        })

        this.initializeModelConfigs()
    }

    /**
     * –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
     */
    private initializeModelConfigs(): void {
        this.modelConfigs = new Map([
            ['player_action_response', {
                modelName: this.config.models.story,
                temperature: 0.8,
                maxTokens: 300,
                topP: 0.9,
                repeatPenalty: 1.1,
                contextWindow: 4096
            }],
            ['scene_description', {
                modelName: this.config.models.story,
                temperature: 0.9,
                maxTokens: 400,
                topP: 0.95,
                repeatPenalty: 1.05,
                contextWindow: 4096
            }],
            ['npc_dialogue', {
                modelName: this.config.models.dialogue,
                temperature: 0.7,
                maxTokens: 200,
                topP: 0.85,
                repeatPenalty: 1.15,
                contextWindow: 2048
            }],
            ['combat_narration', {
                modelName: this.config.models.combat,
                temperature: 0.6,
                maxTokens: 150,
                topP: 0.8,
                repeatPenalty: 1.2,
                contextWindow: 1024
            }],
            ['quest_generation', {
                modelName: this.config.models.quest,
                temperature: 0.8,
                maxTokens: 500,
                topP: 0.9,
                repeatPenalty: 1.0,
                contextWindow: 4096
            }],
            ['story_progression', {
                modelName: this.config.models.story,
                temperature: 0.75,
                maxTokens: 350,
                topP: 0.88,
                repeatPenalty: 1.1,
                contextWindow: 4096
            }],
            ['world_building', {
                modelName: this.config.models.story,
                temperature: 0.85,
                maxTokens: 400,
                topP: 0.92,
                repeatPenalty: 1.05,
                contextWindow: 4096
            }],
            ['random_encounter', {
                modelName: this.config.models.story,
                temperature: 0.9,
                maxTokens: 300,
                topP: 0.95,
                repeatPenalty: 1.1,
                contextWindow: 2048
            }],
            ['consequence_analysis', {
                modelName: this.config.models.story,
                temperature: 0.5,
                maxTokens: 250,
                topP: 0.7,
                repeatPenalty: 1.2,
                contextWindow: 2048
            }]
        ])
    }

    /**
     * –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
     */
    async generateResponse(
        prompt: string,
        requestType: AIRequestType,
        customConfig?: Partial<AIModelConfig>
    ): Promise<AIProcessingResult> {
        const startTime = Date.now()

        try {
            const modelConfig = this.getModelConfig(requestType, customConfig)

            const requestData = {
                model: modelConfig.modelName,
                prompt: prompt,
                options: {
                    temperature: modelConfig.temperature,
                    top_p: modelConfig.topP,
                    repeat_penalty: modelConfig.repeatPenalty,
                    num_predict: modelConfig.maxTokens
                },
                stream: false
            }

            console.log(`ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama: ${requestType}`)

            const response = await this.makeRequestWithRetries(requestData)
            const processingTime = Date.now() - startTime

            const aiResponse: AIResponse = {
                id: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
                requestType,
                content: response.response.trim(),
                metadata: {
                    processingTime,
                    modelUsed: modelConfig.modelName,
                    tokenCount: this.estimateTokenCount(response.response),
                    confidence: this.calculateConfidence(response)
                },
                timestamp: new Date()
            }

            console.log(`‚úÖ Ollama –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ ${processingTime}ms`)

            return {
                success: true,
                response: aiResponse
            }

        } catch (error) {
            console.error('Ollama request failed:', error)

            return {
                success: false,
                error: error instanceof Error ? error.message : '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞',
                retryAfter: this.calculateRetryDelay(error)
            }
        }
    }

    /**
     * –°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤)
     */
    async generateStreamingResponse(
        prompt: string,
        requestType: AIRequestType,
        onChunk: (chunk: string) => void,
        customConfig?: Partial<AIModelConfig>
    ): Promise<AIProcessingResult> {
        const startTime = Date.now()

        try {
            const modelConfig = this.getModelConfig(requestType, customConfig)

            const requestData = {
                model: modelConfig.modelName,
                prompt: prompt,
                options: {
                    temperature: modelConfig.temperature,
                    top_p: modelConfig.topP,
                    repeat_penalty: modelConfig.repeatPenalty,
                    num_predict: modelConfig.maxTokens
                },
                stream: true
            }

            const response = await this.client.post('/api/generate', requestData, {
                responseType: 'stream'
            })

            let fullResponse = ''

            return new Promise((resolve, reject) => {
                response.data.on('data', (chunk: Buffer) => {
                    const lines = chunk.toString().split('\n').filter(line => line.trim())

                    for (const line of lines) {
                        try {
                            const data = JSON.parse(line)
                            if (data.response) {
                                fullResponse += data.response
                                onChunk(data.response)
                            }

                            if (data.done) {
                                const processingTime = Date.now() - startTime

                                const aiResponse: AIResponse = {
                                    id: `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
                                    requestType,
                                    content: fullResponse.trim(),
                                    metadata: {
                                        processingTime,
                                        modelUsed: modelConfig.modelName,
                                        tokenCount: this.estimateTokenCount(fullResponse)
                                    },
                                    timestamp: new Date()
                                }

                                resolve({
                                    success: true,
                                    response: aiResponse
                                })
                            }
                        } catch (parseError) {
                            // –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
                        }
                    }
                })

                response.data.on('error', (error: Error) => {
                    reject({
                        success: false,
                        error: error.message
                    })
                })
            })

        } catch (error) {
            console.error('Ollama streaming request failed:', error)

            return {
                success: false,
                error: error instanceof Error ? error.message : '–û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞'
            }
        }
    }

    /**
     * –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama
     */
    async checkHealth(): Promise<boolean> {
        try {
            const response = await this.client.get('/api/tags', { timeout: 5000 })
            return response.status === 200
        } catch (error) {
            console.error('Ollama health check failed:', error)
            return false
        }
    }

    /**
     * –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
     */
    async getAvailableModels(): Promise<string[]> {
        try {
            const response = await this.client.get('/api/tags')
            return response.data.models?.map((model: any) => model.name) || []
        } catch (error) {
            console.error('Failed to get models:', error)
            return []
        }
    }

    /**
     * –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)
     */
    async loadModel(modelName: string): Promise<boolean> {
        try {
            console.log(`üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: ${modelName}`)

            const response = await this.client.post('/api/generate', {
                model: modelName,
                prompt: 'test',
                options: { num_predict: 1 }
            })

            return response.status === 200
        } catch (error) {
            console.error(`Failed to load model ${modelName}:`, error)
            return false
        }
    }

    /**
     * –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
     */
    async getModelInfo(modelName: string): Promise<any> {
        try {
            const response = await this.client.post('/api/show', { name: modelName })
            return response.data
        } catch (error) {
            console.error(`Failed to get model info for ${modelName}:`, error)
            return null
        }
    }

    // === –ü—Ä–∏–≤–∞—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã ===

    /**
     * –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
     */
    private getModelConfig(
        requestType: AIRequestType,
        customConfig?: Partial<AIModelConfig>
    ): AIModelConfig {
        const baseConfig = this.modelConfigs.get(requestType) || {
            modelName: this.config.defaultModel,
            temperature: 0.7,
            maxTokens: 200,
            topP: 0.9,
            repeatPenalty: 1.1,
            contextWindow: 2048
        }

        return { ...baseConfig, ...customConfig }
    }

    /**
     * –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
     */
    private async makeRequestWithRetries(requestData: any, attempt: number = 1): Promise<any> {
        try {
            const response = await this.client.post('/api/generate', requestData)
            return response.data
        } catch (error) {
            if (attempt < this.config.maxRetries) {
                const delay = Math.pow(2, attempt) * 1000 // –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                console.log(`–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ ${attempt + 1} —á–µ—Ä–µ–∑ ${delay}ms`)

                await new Promise(resolve => setTimeout(resolve, delay))
                return this.makeRequestWithRetries(requestData, attempt + 1)
            }

            throw error
        }
    }

    /**
     * –û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
     */
    private estimateTokenCount(text: string): number {
        // –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        return Math.ceil(text.length / 4)
    }

    /**
     * –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ
     */
    private calculateConfidence(response: any): number {
        // –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞ –∏ –Ω–∞–ª–∏—á–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        const text = response.response || ''

        if (text.length < 10) return 0.3
        if (text.includes('?') && text.length < 50) return 0.5
        if (text.length > 100 && text.includes('.')) return 0.9

        return 0.7
    }

    /**
     * –†–∞—Å—á–µ—Ç –∑–∞–¥–µ—Ä–∂–∫–∏ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
     */
    private calculateRetryDelay(error: any): number {
        if (error?.response?.status === 429) return 30000 // Rate limit
        if (error?.code === 'ECONNREFUSED') return 10000 // Connection refused
        return 5000 // Default delay
    }

    /**
     * –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏
     */
    async clearModelContext(modelName?: string): Promise<boolean> {
        try {
            // Ollama –Ω–µ –∏–º–µ–µ—Ç –ø—Ä—è–º–æ–≥–æ API –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–±—Ä–æ—Å–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            await this.client.post('/api/generate', {
                model: modelName || this.config.defaultModel,
                prompt: '',
                options: { num_predict: 0 }
            })

            return true
        } catch (error) {
            console.error('Failed to clear model context:', error)
            return false
        }
    }

    /**
     * –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
     */
    getUsageStats(): {
        totalRequests: number
        averageResponseTime: number
        errorRate: number
    } {
        // TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        return {
            totalRequests: 0,
            averageResponseTime: 0,
            errorRate: 0
        }
    }

    /**
     * –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
     */
    updateConfig(newConfig: Partial<OllamaConfig>): void {
        this.config = { ...this.config, ...newConfig }

        // –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç —Å –Ω–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        this.client = axios.create({
            baseURL: this.config.baseUrl,
            timeout: this.config.timeout,
            headers: {
                'Content-Type': 'application/json'
            }
        })
    }

    /**
     * –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
     */
    getConfig(): OllamaConfig {
        return { ...this.config }
    }
}